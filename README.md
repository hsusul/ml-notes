# ML Research Notes

A running set of notes and small experiments as I learn machine learning more deeply through papers, coursework, and research.

## Goals
- Build strong intuition for core ML and deep learning ideas
- Practice reading papers and extracting the main contributions
- Reproduce key results when feasible
- Keep a searchable, organized knowledge base over time

## Repository Structure

### `papers/`
Paper summaries and critiques. Each note aims to capture:
- Problem and motivation
- Key idea and contributions
- Core equations and intuition
- Experimental setup and results
- Limitations and follow ups
- My takeaways and open questions

### `concepts/`
Focused notes on underlying math and ML concepts (linear algebra, probability, optimization, etc.).

### `implementations/`
Small, self contained code experiments, reproductions, and sanity checks.

### `resources/`
Reading lists, links, and references I regularly use.

## Note Template (for papers)
Each paper note generally follows this structure:

- Citation
- TLDR
- Problem
- Key idea
- Method
- Results
- What I learned
- Questions / follow ups

## Current Focus
- Deep learning architectures (attention and transformers)
- Optimization and training dynamics
- Evaluation and reliability in high stakes domains

## Disclaimer
These are learning notes. They may include mistakes or incomplete understanding. I revise notes over time as I learn more.

